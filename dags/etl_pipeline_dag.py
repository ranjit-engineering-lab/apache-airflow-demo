# ============================================================
# Constants
# ============================================================
AWS_CONN_ID = 'ensf-aws'
BUCKET_NAME = 'ensf612bucket'
POSTGRES_CONN_ID = 'user_postgres'
TABLE_NAME = 'public.walmart_sales_transformed'
LOCAL_DATA_DIR = '/opt/airflow/data/extracted'

# ============================================================
# Imports
# ============================================================
from airflow import DAG
from airflow.decorators import task
from airflow.sensors.base import PokeReturnValue
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.operators.empty import EmptyOperator
from airflow.utils.trigger_rule import TriggerRule
from airflow.operators.python import BranchPythonOperator

import os
import csv
import pandas as pd
from datetime import datetime
from airflow.providers.amazon.aws.hooks.s3 import S3Hook

# ============================================================
# Task: ExtractFromS3
# ============================================================
@task(task_id="ExtractFromS3")
def extract_from_s3():
    os.makedirs(LOCAL_DATA_DIR, exist_ok=True)
    local_file_path = f'{LOCAL_DATA_DIR}/walmart_sales.csv'

    if os.path.exists(local_file_path):
      os.remove(local_file_path)
    
    s3 = S3Hook(aws_conn_id=AWS_CONN_ID)
    s3.download_file(
        key="walmart_sales.csv",
        bucket_name=BUCKET_NAME,
        local_path=LOCAL_DATA_DIR,
        preserve_file_name=True,
        use_autogenerated_subdir=False
    )
    return local_file_path

# ============================================================
# Task: FileAvailabilitySensor
# ============================================================
@task.sensor(
    task_id="FileAvailabilitySensor",
    poke_interval=30,
    timeout=3600,
    mode="poke"
)
def file_availability_sensor(file_path: str):
    if not os.path.exists(file_path):
        return PokeReturnValue(is_done=False)
    try:
        with open(file_path, "r") as f:
            reader = csv.reader(f)
            rows = list(reader)
            if len(rows) > 1:  # has data
                return PokeReturnValue(is_done=True, xcom_value=file_path)
    except Exception:
        return PokeReturnValue(is_done=False)
    return PokeReturnValue(is_done=False)

# ============================================================
# Task: BranchingTask
# ============================================================
def branching_task_func(ti):
    """
    Branching logic:
    - Returns "TransformData" if CSV has rows
    - Returns "ErrorReport" if CSV is empty or missing
    """
    # Pull file path from sensor task XCom
    file_path = ti.xcom_pull(task_ids='FileAvailabilitySensor')

    if not file_path or not os.path.exists(file_path):
        return "ErrorReport"

    with open(file_path, "r") as f:
        reader = csv.reader(f)
        rows = list(reader)
        if len(rows) > 1:  # header + at least 1 data row
            return "TransformData"
        else:
            return "ErrorReport"

# ============================================================
# Task: TransformData
# ============================================================
@task(task_id="TransformData")
def transform_data():
    file_path = os.path.join(LOCAL_DATA_DIR, "walmart_sales.csv")
    

    df = pd.read_csv(file_path)
    df = df.dropna()
    df['Date'] = pd.to_datetime(df['Date'])

    markdown_cols = ["MarkDown1", "MarkDown2", "MarkDown3", "MarkDown4", "MarkDown5"]
    df['promo_intensity'] = df[markdown_cols].sum(axis=1)
    df.columns = [col.lower().replace(' ', '_') for col in df.columns]

    df_transformed = df.groupby(['store', 'date']).agg({
        'temperature': 'mean',
        'fuel_price': 'mean',
        'promo_intensity': 'sum',
        'isholiday': 'max',
        'cpi': 'mean',
        'unemployment': 'mean'
    }).reset_index()

    cleaned_path = os.path.join(LOCAL_DATA_DIR, "walmart_sales_transformed.csv")
    df_transformed['isholiday'] = df_transformed['isholiday'].astype(int)    
    df_transformed.to_csv(cleaned_path, index=False)
    return cleaned_path

# ============================================================
# Task: ErrorReport
# ============================================================
@task(task_id="ErrorReport")
def error_report():
    return {"message": "Errors detected in the ETL process.", "status_code": 0}

# ============================================================
# Task: CreateTable
# ============================================================
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator


def create_table_task(): 
    
    return SQLExecuteQueryOperator(
    task_id="CreateTable",
    conn_id=POSTGRES_CONN_ID,
    sql=f"""
    CREATE TABLE IF NOT EXISTS {TABLE_NAME} (
        store INT,
        date DATE,
        temperature FLOAT,
        fuel_price FLOAT,
        promo_intensity INT,
        isholiday INT,
        cpi FLOAT,
        unemployment FLOAT
    );
    """
)


# ============================================================
# Task: LoadData
# ============================================================
@task(task_id="LoadData")
def load_data():
    file_path = os.path.join(LOCAL_DATA_DIR, "walmart_sales_transformed.csv")
    df = pd.read_csv(file_path)

    pg_hook = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)
    engine = pg_hook.get_sqlalchemy_engine()

    df.to_sql(
        name=TABLE_NAME.split('.')[-1],
        schema=TABLE_NAME.split('.')[0],
        con=engine,
        if_exists="append",
        index=False
    )
    return {"message": "Data loaded successfully into Postgres.", "status_code": 1}

# ============================================================
# Task: Notification
# ============================================================
@task(
    task_id="Notification",
    trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS
)
def notification(load_data_result: dict = None, error_report_result: dict = None):
    report = load_data_result or error_report_result
    if report:
        print(report["message"])
    else:
        print("No report available.")
    return report

# ============================================================
# DAG Definition
# ============================================================
with DAG(
    dag_id="A4_etl_pipeline_dag",
    schedule="@daily",
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=["etl"]
) as dag:

    # DAG Tasks
    extracted_file = extract_from_s3()
    sensor_file = file_availability_sensor(extracted_file)
    branch = BranchPythonOperator(
        task_id="BranchingTask",
        python_callable=branching_task_func
    )

    transformed_file = transform_data()
    error_report_task = error_report()

    create_table = create_table_task()
    load_task = load_data()
    notification_task = notification(load_data_result=load_task, error_report_result=error_report_task)

    # ============================================================
    # DAG Dependencies
    # ============================================================
    extracted_file >> sensor_file >> branch
    load_task << create_table
    branch >> transformed_file >> load_task >> notification_task
    branch >> error_report_task >> notification_task
    
